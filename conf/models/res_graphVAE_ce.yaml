name: DenseGraphVAE
_target_: maze_representations.models.graphVAE.LightningGraphVAE

# these parameters should not change
configuration:

  shared_parameters:
    latent_dim: 1024 #64
    graph_max_nodes: ${data.dataset.max_nodes}
    node_attributes: ${data.dataset.node_attributes}
    batch_size: ${data.dataset.batch_size}
    data_encoding: ${data.dataset.encoding}
    gridworld_data_dim: ${data.dataset.gridworld_data_dim}
    dropout: 0.0
    use_batch_norm: true

  model:
    outputs: [
      "loss",
      "elbos",
      "unweighted_elbos",
      "neg_cross_entropy_Fx",
      "kld",
      "logits_Fx",
      "std",
      "mean",
    ]
    accelerator: ${accelerator}
    gradient_type: pathwise
    num_variational_samples: 1
    augmented_inputs: false
    augmented_transforms: null
#    augmented_transforms: [[[1, 0], [0, 1]],
#                           [[1, 0], [0, -1]],
#                           [[0, 1], [1, 0]],
#                           [[0, 1], [-1, 0]],
#                           [[-1, 0], [0, 1]],
#                           [[-1, 0], [0, -1]],
#                           [[0, -1], [1, 0]],
#                           [[0, -1], [-1, 0]]]

  encoder:
    attributes: ["lava", "moss", "wall", "empty", "start", "goal"]
    #attributes_mapping: [0, 2, 3]
    gnn:
      architecture: GIN
      num_layers: 4 #8
      layer_dim: 12
      num_mlp_layers: 2
      learn_eps: true
      graph_pooling: mean
      neighbor_pooling: sum

    mlp:
      num_layers: 1
      hidden_dim: 2048
      bottleneck_dim: 256
      batch_norm_output_layer: true
      dropout_output_layer: false

  decoder:
    adjacency: null #[full, reduced, null]
    attributes: ["lava", "moss", "wall", "empty", "start", "goal"]
    attribute_masking: always #[null, always, gen_only]
    #attributes_mapping: [2, 3] #predict start, goal
    distributions: ["bernoulli", "one_hot_categorical", "one_hot_categorical"]
    #distributions_domains: ["node", "nodeset", "nodeset"] #Not implemented
    output_dim: #this should be computed according to adjacency, data.graph_max_nodes
      adjacency: null
      attributes: ${data.dataset.max_nodes}
    architecture: MLP
    num_layers: 2 #not counting the final multihead layer
    bottleneck_dim: 256 #not counting the final multihead layer
    hidden_dim: 256 #not counting the final multihead layer
    batch_norm_output_layer: true
    dropout_output_layer: false

  predictor:
    enable: False
    target_metric: resistance_distance
    target_from: output
    target_transform: log # log or identity
    num_layers: 1
    hidden_dim: 8 #n/a if num_layer = 1
    alpha_reg: 0.0


# these can change between runs
hyperparameters:
  loss:
    elbo_coeffs:
      A: 0.
      Fx:
        active: 0.04
        start: 0.013
        goal: ${models.hyperparameters.loss.elbo_coeffs.Fx.start}
      beta: 0.0448
      predictor: 0.