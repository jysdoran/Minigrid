name: GraphVAE
_target_: models.graphVAE.LightningGraphVAE

# these parameters should not change
configuration:

  shared_parameters:
    latent_dim: 12 #64
    graph_max_nodes: ${data.dataset.max_nodes}
    node_attributes: ${data.dataset.node_attributes}
    batch_size: ${data.dataset.batch_size}

  model:
    accelerator: ${accelerator}
    gradient_type: pathwise
    num_variational_samples: 1
    augmented_inputs: False
    augmented_transforms: null
#    augmented_transforms: [[[1, 0], [0, 1]],
#                           [[1, 0], [0, -1]],
#                           [[0, 1], [1, 0]],
#                           [[0, 1], [-1, 0]],
#                           [[-1, 0], [0, 1]],
#                           [[-1, 0], [0, -1]],
#                           [[0, -1], [1, 0]],
#                           [[0, -1], [-1, 0]]]

  encoder:
    attributes: ["empty", "start", "goal"]
    #attributes_mapping: [0, 2, 3]
    gnn:
      architecture: GIN
      num_layers: 5 #8
      layer_dim: 8
      num_mlp_layers: 2
      final_dropout: 0
      learn_eps: False
      graph_pooling: mean
      neighbor_pooling: sum

    mlp:
      num_layers: 2
      hidden_dim: 1024
      bottleneck_dim: 128

  decoder:
    adjacency: reduced #[full, reduced, null]
    attributes: ["start", "goal"]#["empty", "start", "goal"]
    #attributes_mapping: [2, 3] #predict start, goal
    distributions: ["one_hot_categorical", "one_hot_categorical"]
    distributions_domains: ["nodes", "nodes"]
    output_dim: #this should be computed according to adjacency, data.graph_max_nodes
      adjacency: null # Auto (max_node-1)*2
      attributes: ${data.dataset.max_nodes}
    architecture: MLP
    num_layers: 2 #not counting the final multihead layer
    bottleneck_dim: 128 #not counting the final multihead layer
    hidden_dim: 128 #not counting the final multihead layer

# these can change between runs
hyperparameters:
  loss:
    elbo_coeffs:
      A: 1
      Fx: .25
      beta: 1